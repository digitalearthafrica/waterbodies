{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ba83a6-f2bd-47e6-9f02-8aa361b3e05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Path to env file containing the waterbodies database credentials\n",
    "# Only necessary on the Sandbox.\n",
    "dotenv_path = \"/home/jovyan/.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path, verbose=True, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00a3d0c-81cf-49ee-b0dc-14759b0989b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"2024-04-12--P7D\""
     ]
    }
   ],
   "source": [
    "# Temporal range is defined in the Argo workflow generate-temporal-range step\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "today = datetime.now().date()\n",
    "seven_days_ago = today - timedelta(days=7)\n",
    "temporal_range = f\"{seven_days_ago:%Y-%m-%d}--P7D\"\n",
    "json.dump(temporal_range, sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f899b0-02a9-4454-9e9e-9c40fe5423fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from itertools import groupby\n",
    "\n",
    "import click\n",
    "import numpy as np\n",
    "from datacube import Datacube\n",
    "from odc.stats.model import DateTimeRange\n",
    "\n",
    "from waterbodies.hopper import create_tasks_from_datasets\n",
    "from waterbodies.io import check_directory_exists, find_geotiff_files, get_filesystem\n",
    "from waterbodies.logs import logging_setup\n",
    "from waterbodies.text import format_task, get_tile_id_tuple_from_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58450a47-5c9c-474c-acee-15743b46b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 3\n",
    "run_type = \"backlog-processing\"\n",
    "temporal_range\n",
    "historical_extent_rasters_directory = (\n",
    "    \"s3://deafrica-waterbodies-dev/waterbodies/v0.0.2/historical_extent_rasters/\"\n",
    ")\n",
    "max_parallel_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f520655-8531-4dc1-8e0e-fdde4cad9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging.\n",
    "logging_setup(verbose)\n",
    "_log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8b2648-aba7-4d60-9d0a-c4b044b4e133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:51:52,372] {credentials.py:557} INFO - Found credentials in environment variables.\n"
     ]
    }
   ],
   "source": [
    "if not check_directory_exists(path=historical_extent_rasters_directory):\n",
    "    e = FileNotFoundError(f\"Directory {historical_extent_rasters_directory} does not exist!\")\n",
    "    _log.error(e)\n",
    "    raise e\n",
    "else:\n",
    "    historical_extent_rasters = find_geotiff_files(\n",
    "        directory_path=historical_extent_rasters_directory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd4e3a35-885b-487c-a154-ef106659d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tile_ids for tiles that actually contain waterbodies.\n",
    "tiles_containing_waterbodies = [\n",
    "    get_tile_id_tuple_from_filename(file_path=raster_file)\n",
    "    for raster_file in historical_extent_rasters\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f31425-7311-4970-9c87-1d92018abcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"wofs_ls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee21ed18-a651-4f1f-b798-9eba445e1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the temporal range\n",
    "temporal_range_ = DateTimeRange(temporal_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "289e47fe-c418-468c-a22a-e6b79bb82eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the datacube\n",
    "dc = Datacube(app=run_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a5e4b4-bd44-404c-a01c-46bb451ebdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if abs(temporal_range_.end - temporal_range_.start) > timedelta(days=7):\n",
    "    _log.warning(\n",
    "        \"Gap-filling is only meant to be run for a temporal range of 7 days or less. \"\n",
    "        \"If running for a larger temporal range please except long run times.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a672ce-6934-4c0d-b8ad-9e644946e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The difference between gap-filling and the other steps is here\n",
    "# we are seatemporal_rangerching for datasets by their creation date (`creation_time`),\n",
    "# not their acquisition date (`time`).\n",
    "dc_query_ = dict(product=product, creation_time=(temporal_range_.start, temporal_range_.end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46faf08f-83d8-404f-a6b9-9402bf045d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:52:05,478] {<timed exec>:6} INFO - Found 1128 datasets matching the query {'product': 'wofs_ls', 'creation_time': (datetime.datetime(2024, 4, 12, 0, 0), datetime.datetime(2024, 4, 18, 23, 59, 59, 999999))}\n",
      "CPU times: user 75.6 ms, sys: 13.6 ms, total: 89.3 ms\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Search the datacube for all wofs_ls datasets whose creation times (not acquisition time)\n",
    "# fall within the temporal range specified.\n",
    "# E.g  a dataset can have an aquisition date of 2023-12-15 but have been added to the\n",
    "# datacube in 2024-02, which will be its creation date.\n",
    "datasets_ = dc.find_datasets(**dc_query_)\n",
    "_log.info(f\"Found {len(datasets_)} datasets matching the query {dc_query_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e333906-9abf-4b03-8239-5a00f493e2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1128 datasets: 100%|██████████| 1128/1128 [00:01<00:00, 611.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 24.4 ms, total: 2 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get the ids of the tasks to process.\n",
    "tasks_ = create_tasks_from_datasets(\n",
    "    datasets=datasets_, tile_ids_of_interest=tiles_containing_waterbodies\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4d57484-c7f0-4089-b555-8f7e00df3d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update each task with the datasets whose acquisition time matches\n",
    "# the solar day in the task id.\n",
    "task_ids_ = [task_id for task in tasks_ for task_id, task_dataset_ids in task.items()]\n",
    "# Sort the task ids by the solar day.\n",
    "sorted_task_ids = sorted(task_ids_, key=lambda x: x[0])\n",
    "# Group the sorted task ids by solar day.\n",
    "grouped_task_ids = {key: list(group) for key, group in groupby(sorted_task_ids, key=lambda x: x[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a41b05f-911a-48d9-b1e6-e3b20fb1ed4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:52:07,484] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-04  1/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 183 datasets: 100%|██████████| 183/183 [00:00<00:00, 506.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:52:52,907] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-05  2/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 185 datasets: 100%|██████████| 185/185 [00:00<00:00, 509.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:53:30,327] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-06  3/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 174 datasets: 100%|██████████| 174/174 [00:00<00:00, 509.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:54:05,365] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-07  4/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 169 datasets: 100%|██████████| 169/169 [00:00<00:00, 366.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:54:40,514] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-08  5/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 175 datasets: 100%|██████████| 175/175 [00:00<00:00, 506.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:55:16,974] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-09  6/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 179 datasets: 100%|██████████| 179/179 [00:00<00:00, 504.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:55:51,840] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-11  7/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 180 datasets: 100%|██████████| 180/180 [00:00<00:00, 507.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:56:26,865] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-12  8/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 183 datasets: 100%|██████████| 183/183 [00:00<00:00, 504.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:57:01,701] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-13  9/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 185 datasets: 100%|██████████| 185/185 [00:00<00:00, 519.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:57:37,522] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-14  10/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 89 datasets: 100%|██████████| 89/89 [00:00<00:00, 299.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:58:19,247] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-15  11/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 84 datasets: 100%|██████████| 84/84 [00:00<00:00, 503.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:59:07,225] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-16  12/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 92 datasets: 100%|██████████| 92/92 [00:00<00:00, 512.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 19:59:44,074] {<timed exec>:4} INFO - Updating datasets for tasks with the solar day: 2024-04-17  13/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 86 datasets: 100%|██████████| 86/86 [00:00<00:00, 504.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 75.4 ms, total: 4.68 s\n",
      "Wall time: 8min 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tasks = []\n",
    "idx = 1\n",
    "for solar_day, task_ids in grouped_task_ids.items():\n",
    "    _log.info(\n",
    "        f\"Updating datasets for tasks with the solar day: {solar_day}  {idx}/{len(grouped_task_ids)}\"\n",
    "    )\n",
    "    task_tile_ids = [(task_id[1], task_id[2]) for task_id in task_ids]\n",
    "    dc_query = dict(product=product, time=(solar_day))\n",
    "    datasets = dc.find_datasets(**dc_query)\n",
    "    updated_tasks = create_tasks_from_datasets(\n",
    "        datasets=datasets, tile_ids_of_interest=task_tile_ids\n",
    "    )\n",
    "    tasks.extend(updated_tasks)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2c429f3-79c8-4412-8adf-157b9e6f0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the tasks in the correct format.\n",
    "tasks = [format_task(task) for task in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8406a4c9-387d-4fcc-bffa-d299e71aaf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 20:00:26,585] {2708203668.py:3} INFO - Total number of tasks: 4186\n"
     ]
    }
   ],
   "source": [
    "# Sort the tasks by solar day\n",
    "sorted_tasks = sorted(tasks, key=lambda x: x[\"solar_day\"])\n",
    "_log.info(f\"Total number of tasks: {len(sorted_tasks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecfa59b8-c49e-43e9-ae2c-e48e003b1fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the list into chunks.\n",
    "task_chunks = np.array_split(np.array(sorted_tasks), max_parallel_steps)\n",
    "task_chunks = [chunk.tolist() for chunk in task_chunks]\n",
    "# Remove empty lists\n",
    "task_chunks = list(filter(None, task_chunks))\n",
    "# Get the number of chunks.\n",
    "task_chunks_count = str(len(task_chunks))\n",
    "task_chunks_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26cc1d09-aa30-4fdb-962a-1bc500741b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to json array.\n",
    "task_chunks_json_array = json.dumps(task_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edbd7ea1-c8f6-46d0-b23e-592cdfc7355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_directory = \"/tmp/\"\n",
    "tasks_output_file = os.path.join(tasks_directory, \"tasks\")\n",
    "tasks_count_file = os.path.join(tasks_directory, \"tasks_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06151dcb-d44a-42d1-831e-075e327b08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = get_filesystem(path=tasks_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cac7984-1826-46bb-999d-6d0f00ea3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_directory_exists(path=tasks_directory):\n",
    "    fs.mkdirs(path=tasks_directory, exist_ok=True)\n",
    "    _log.info(f\"Created directory {tasks_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d38306a1-9eae-41b5-b184-637dfb267b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-19 20:00:26,627] {1726518394.py:3} INFO - Tasks written to /tmp/tasks\n",
      "[2024-04-19 20:00:26,628] {1726518394.py:7} INFO - Tasks count written to /tmp/tasks_count\n"
     ]
    }
   ],
   "source": [
    "with fs.open(tasks_output_file, \"w\") as file:\n",
    "    file.write(task_chunks_json_array)\n",
    "_log.info(f\"Tasks written to {tasks_output_file}\")\n",
    "\n",
    "with fs.open(tasks_count_file, \"w\") as file:\n",
    "    file.write(task_chunks_count)\n",
    "_log.info(f\"Tasks count written to {tasks_count_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
